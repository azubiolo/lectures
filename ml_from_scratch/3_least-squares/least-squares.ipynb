{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary Least Squares\n",
    "## Course recap\n",
    "This lab consists in implementing the **Ordinary Least Squares** (OLS) algorithm, which is **a linear regression with a least-squares penalty**. Given a training set $ D = \\left\\{ \\left(x^{(i)}, y^{(i)}\\right), x^{(i)} \\in \\mathcal{X}, y^{(i)} \\in \\mathcal{Y}, i \\in \\{1, \\dots, n \\}  \\right\\}$, recall (from lectures 1 and 2) OLS aims at minimizing the following cost function $J$:\n",
    "$$J(\\theta) = \\dfrac{1}{2} \\sum_{i = 1}^{n} \\left( h\\left(x^{(i)}\\right) - y^{(i)} \\right)^2$$\n",
    "where \n",
    "$$h(x) = \\sum_{j = 0}^{d} \\theta_j x_j = \\theta^T x.$$\n",
    "\n",
    "For the sake of simplicity, we will be working on a small training set (the one we used in lectures 1 and 2) $D$:\n",
    "\n",
    "| living area (m$^2$) | price (1000's BGN)|\n",
    "|--------------------:|------------------:|\n",
    "|                 50  |         30        |\n",
    "|                 76  |         48        |\n",
    "|                 26  |         12        |\n",
    "|                102  |         90        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the training set\n",
    "**Exercise 1**: Define variables that will contain the training set.\n",
    "\n",
    "**Note**: Do not forget the intercept!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = [[50., 1.], [76., 1.], [26., 1.], [102., 1.]]\n",
    "Y = [30., 48., 12., 90.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this very simple example, the dimensionality is $d = 1$ and the number of samples is $n = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction function\n",
    "**Exercise**: Define a function `predict` that takes as parameter *the feature vector* $x$ and *the model* $\\theta$ and returns the predicted label:\n",
    "$$ \\hat{y} = h(x) = \\theta^T x = \\sum_{j = 0}^d \\theta_j x_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(x, theta):\n",
    "    y_hat = x[0] * theta[0] + x[1] * theta[1]\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the cost function\n",
    "### Cost function on a single sample\n",
    "**Exercise**: Define a function `cost_function` that takes as parameter *the predicted label* $y$ and *the actual label* $\\hat{y}$ of a single sample and returns the loss of this pair given by:\n",
    "$$ \\ell \\left( y - \\hat{y} \\right) = \\dfrac{1}{2}\\left( y - \\hat{y} \\right)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_function(y, y_hat):\n",
    "    loss = (y - y_hat) ** 2 / 2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function on the whole training set\n",
    "Now we are able to compute the cost function for a single sample, we can easily compute the cost function for the whole training set by summing the cost function values for all the samples in the training set. Recall that the total cost function is given by:\n",
    "$$J(\\theta) = \\dfrac{1}{2} \\sum_{i = 1}^{n} \\left( h\\left(x^{(i)}\\right) - y^{(i)} \\right)^2$$\n",
    "where, for all $i \\in \\{ 1, \\dots, n \\}$, we have:\n",
    "$$h\\left(x^{(i)}\\right) = \\sum_{j = 0}^{d} \\theta_j x^{(i)}_j = \\theta^T x.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_function_total(X, Y, theta):\n",
    "    cost = 0 # initialize the cost with 0\n",
    "    n = len(Y)\n",
    "    for i in range(n):\n",
    "        x = X[i] # get the ith feature vector\n",
    "        y = Y[i] # get the ith label\n",
    "        y_hat = predict(x, theta) # predict the ith label\n",
    "        cost += cost_function(y, y_hat) # add the cost for the current sample to the total cost\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test the code written above and check the total cost function we would have when $\\theta = [0, 0]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5724.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_0 = [0, 0]\n",
    "cost_function_total(X, Y, theta_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the gradient of the cost function\n",
    "### Gradient on a single sample\n",
    "**Exercise**: Define a function `gradient` that implements the gradient of the cost function for a given sample $(x, y)$. Recall from the lectures 1 and 2 that the gradient is given by:\n",
    "$$\\nabla J(\\theta) = \\left[ \\dfrac{\\partial}{\\partial \\theta_1} J(\\theta), \\dots, \\dfrac{\\partial}{\\partial \\theta_d} J(\\theta) \\right]^T$$\n",
    "where, for all $j \\in \\{0, \\dots, d \\}$:\n",
    "$$ \\dfrac{\\partial}{\\partial \\theta_j} J(\\theta) = \\left( h\\left(x\\right) - y \\right) x_j $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient(x, y, theta):\n",
    "    grad = [0, 0]\n",
    "    grad[0] = (predict(x, theta) - y) * x[0]\n",
    "    grad[1] = (predict(x, theta) - y) * x[1]\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our application, the dimensionality of our data (with the intercept) is 2, so the gradient has 2 values (corresponding to $\\theta_0$ and $\\theta_1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1500.0, -30.0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient(X[0], Y[0], theta_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient on the whole training set\n",
    "Now we are able to compute the gradient of the cost function on a single sample, we can easily compute `gradient_total`, the gradient of the cost function on the whole training set by summing the gradients for all the samples in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_total(X, Y, theta):\n",
    "    grad_total = [0, 0] # initialize the gradient with zeros\n",
    "    n = len(Y)\n",
    "    for i in range(n):\n",
    "        x = X[i] # get the ith feature vector\n",
    "        y = Y[i] # get the ith label\n",
    "        grad = gradient(x, y, theta) # predict the ith label\n",
    "        grad_total[0] += grad[0] # add the gradient corresponding to theta[0]\n",
    "        grad_total[1] += grad[1] # add the gradient corresponding to theta[1]\n",
    "    return grad_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test the code written above and check the total gradient we would have when $\\theta = [0, 0]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-14640.0, -180.0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_total(X, Y, theta_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What is the sign of the gradient value? What would it mean if we had such a gradient when applying a gradient descent?\n",
    "\n",
    "## Applying a gradient descent\n",
    "We now have all the building blocs needed for the gradient descent algorithm, that is:\n",
    "- The loss function\n",
    "- The gradient\n",
    "Indeed, the iterative update scheme of this algorithm is given by the following formula:\n",
    "$$\\theta_j := \\theta_j - \\alpha \\dfrac{\\partial}{\\partial \\theta_j} J(\\theta)$$\n",
    "for all $j \\in \\{0, \\dots, d \\}$. Recall that $\\alpha$ is a parameter called the *learning rate* (or *step size*).\n",
    "\n",
    "**Exercise**: Define a function called `gradient_descent_step` that performs an update on theta by applying the formula above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent_step(X, Y, theta, alpha):\n",
    "    theta_updated = [0, 0]\n",
    "    grad = gradient_total(X, Y, theta)\n",
    "    theta_updated[0] = theta[0] - alpha * grad[0]\n",
    "    theta_updated[1] = theta[1] - alpha * grad[1]\n",
    "    return theta_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0938243999999997, -0.0011927999999999973]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.0001\n",
    "theta_1 = gradient_descent_step(X, Y, theta_0, alpha)\n",
    "theta_2 = gradient_descent_step(X, Y, theta_1, alpha)\n",
    "theta_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
